Running your FastAPI server, Prometheus, Grafana, and the external database in Docker containers is a great way to manage dependencies and isolate each service. Here's a detailed guide on setting up these services using Docker and Docker Compose.

### Step 1: Create Dockerfiles

#### FastAPI Dockerfile
Create a `Dockerfile` for the FastAPI application.

```Dockerfile
# Dockerfile for FastAPI application
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Prometheus Configuration
Create a `prometheus.yml` configuration file.

```yaml
# prometheus.yml
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['fastapi:8000']
```

#### Grafana Configuration
For Grafana, you don't need a specific configuration file initially. You will configure data sources through the Grafana UI after setup.

### Step 2: Create a Docker Compose File

Create a `docker-compose.yml` file to define and run multi-container Docker applications.

```yaml
version: '3.8'

services:
  fastapi:
    build: ./fastapi
    container_name: fastapi
    ports:
      - "8000:8000"
    depends_on:
      - postgres
    environment:
      - POSTGRES_DB=metricsdb
      - POSTGRES_USER=metricsuser
      - POSTGRES_PASSWORD=yourpassword
      - POSTGRES_HOST=postgres

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - postgres

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_DB: metricsdb
      POSTGRES_USER: metricsuser
      POSTGRES_PASSWORD: yourpassword
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### Step 3: Modify FastAPI to Use PostgreSQL

Modify the FastAPI code to connect to the PostgreSQL container using environment variables.

```python
# main.py for FastAPI application
from fastapi import FastAPI, HTTPException
from prometheus_client import Counter, Histogram, generate_latest
import boto3
import time
import psycopg2
import os

app = FastAPI()

# Set up Prometheus metrics
DOWNLOAD_COUNTER = Counter('file_downloads', 'Number of files downloaded')
DOWNLOAD_SIZE_HISTOGRAM = Histogram('file_download_size', 'Size of files downloaded in bytes')
DOWNLOAD_DURATION_HISTOGRAM = Histogram('file_download_duration', 'Duration of file download in seconds')

# Set up S3 client
s3_client = boto3.client('s3')

# Set up PostgreSQL connection
conn = psycopg2.connect(
    dbname=os.getenv("POSTGRES_DB"),
    user=os.getenv("POSTGRES_USER"),
    password=os.getenv("POSTGRES_PASSWORD"),
    host=os.getenv("POSTGRES_HOST")
)
cursor = conn.cursor()

@app.on_event("startup")
async def startup_event():
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS downloads (
            id SERIAL PRIMARY KEY,
            file_key TEXT,
            file_size BIGINT,
            duration DOUBLE PRECISION,
            timestamp TIMESTAMPTZ DEFAULT NOW()
        )
    """)
    conn.commit()

@app.get("/download/{bucket_name}/{file_key}")
async def download_file(bucket_name: str, file_key: str):
    try:
        start_time = time.time()
        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)
        file_content = response['Body'].read()
        file_size = len(file_content)
        duration = time.time() - start_time

        # Update Prometheus metrics
        DOWNLOAD_COUNTER.inc()
        DOWNLOAD_SIZE_HISTOGRAM.observe(file_size)
        DOWNLOAD_DURATION_HISTOGRAM.observe(duration)

        # Store metrics in PostgreSQL
        cursor.execute(
            "INSERT INTO downloads (file_key, file_size, duration) VALUES (%s, %s, %s)",
            (file_key, file_size, duration)
        )
        conn.commit()

        return {"file_key": file_key, "file_size": file_size, "duration": duration}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def metrics():
    return generate_latest()
```

### Step 4: Build and Run the Docker Containers

1. **Create the directory structure:**
   ```
   project-root/
   ├── fastapi/
   │   ├── Dockerfile
   │   ├── main.py
   │   ├── requirements.txt
   ├── prometheus/
   │   ├── prometheus.yml
   ├── docker-compose.yml
   ```

2. **Build and run the containers:**
   ```bash
   docker-compose up --build
   ```

### Step 5: Configure Grafana

1. **Open Grafana:**
   - Access Grafana at `http://localhost:3000`.
   - Default credentials: `admin`/`admin`.

2. **Add Prometheus as a data source:**
   - Go to Configuration -> Data Sources -> Add data source.
   - Select Prometheus.
   - Set the URL to `http://prometheus:9090` and save.

3. **Add PostgreSQL as a data source:**
   - Go to Configuration -> Data Sources -> Add data source.
   - Select PostgreSQL.
   - Set the host to `postgres:5432`, database to `metricsdb`, user to `metricsuser`, and password to `yourpassword`.

### Monitoring and Visualization

- **Prometheus:** Collects and stores metrics from the FastAPI server.
- **Grafana:** Visualizes metrics from Prometheus and PostgreSQL.
- **PostgreSQL:** Stores detailed metrics about the downloads.

This setup allows you to run each component in its own Docker container, ensuring isolation and ease of deployment. You can scale and manage each service independently.
